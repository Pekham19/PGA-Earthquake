{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff04d1dd-0a87-4bbd-b32c-9c0894aa9fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Events used: 284,368\n",
      "Calculating distances for earthquakes...\n",
      "Distance calculation for earthquakes completed.\n",
      "Grid: 90 x 180 cells\n",
      "Starting PGA computation...\n",
      "Processed 0 events\n",
      "Processed 1000 events\n",
      "Processed 6000 events\n",
      "Processed 8000 events\n",
      "Processed 9000 events\n",
      "Processed 12000 events\n",
      "Processed 13000 events\n",
      "Processed 16000 events\n",
      "Processed 19000 events\n",
      "Processed 20000 events\n",
      "Processed 21000 events\n",
      "Processed 22000 events\n",
      "Processed 23000 events\n",
      "Processed 25000 events\n",
      "Processed 26000 events\n",
      "Processed 27000 events\n",
      "Processed 28000 events\n",
      "Processed 35000 events\n",
      "Processed 37000 events\n",
      "Processed 47000 events\n",
      "Processed 48000 events\n",
      "Processed 50000 events\n",
      "Processed 51000 events\n",
      "Processed 52000 events\n",
      "Processed 53000 events\n",
      "Processed 55000 events\n",
      "Processed 56000 events\n",
      "Processed 59000 events\n",
      "Processed 62000 events\n",
      "Processed 65000 events\n",
      "Processed 71000 events\n",
      "Processed 73000 events\n",
      "Processed 83000 events\n",
      "Processed 87000 events\n",
      "Processed 89000 events\n",
      "Processed 90000 events\n",
      "Processed 91000 events\n",
      "Processed 93000 events\n",
      "Processed 94000 events\n",
      "Processed 97000 events\n",
      "Processed 98000 events\n",
      "Processed 99000 events\n",
      "Processed 104000 events\n",
      "Processed 107000 events\n",
      "Processed 108000 events\n",
      "Processed 111000 events\n",
      "Processed 113000 events\n",
      "Processed 114000 events\n",
      "Processed 115000 events\n",
      "Processed 116000 events\n",
      "Processed 122000 events\n",
      "Processed 124000 events\n",
      "Processed 128000 events\n",
      "Processed 132000 events\n",
      "Processed 139000 events\n",
      "Processed 140000 events\n",
      "Processed 141000 events\n",
      "Processed 147000 events\n",
      "Processed 150000 events\n",
      "Processed 151000 events\n",
      "Processed 155000 events\n",
      "Processed 157000 events\n",
      "Processed 160000 events\n",
      "Processed 162000 events\n",
      "Processed 164000 events\n",
      "Processed 166000 events\n",
      "Processed 167000 events\n",
      "Processed 171000 events\n",
      "Processed 172000 events\n",
      "Processed 173000 events\n",
      "Processed 175000 events\n",
      "Processed 176000 events\n",
      "Processed 178000 events\n",
      "Processed 179000 events\n",
      "Processed 180000 events\n",
      "Processed 183000 events\n",
      "Processed 184000 events\n",
      "Processed 185000 events\n",
      "Processed 189000 events\n",
      "Processed 191000 events\n",
      "Processed 194000 events\n",
      "Processed 195000 events\n",
      "Processed 196000 events\n",
      "Processed 199000 events\n",
      "Processed 213000 events\n",
      "Processed 217000 events\n",
      "Processed 220000 events\n",
      "Processed 223000 events\n",
      "Processed 225000 events\n",
      "Processed 228000 events\n",
      "Processed 230000 events\n",
      "Processed 233000 events\n",
      "Processed 236000 events\n",
      "Processed 237000 events\n",
      "Processed 239000 events\n",
      "Processed 240000 events\n",
      "Processed 249000 events\n",
      "Processed 253000 events\n",
      "Processed 264000 events\n",
      "Processed 265000 events\n",
      "Processed 266000 events\n",
      "Processed 276000 events\n",
      "Processed 278000 events\n",
      "Processed 286000 events\n",
      "Processed 289000 events\n",
      "Processed 290000 events\n",
      "Processed 296000 events\n",
      "Processed 297000 events\n",
      "Processed 305000 events\n",
      "Processed 311000 events\n",
      "Processed 316000 events\n",
      "Processed 320000 events\n",
      "Processed 332000 events\n",
      "Processed 337000 events\n",
      "Processed 338000 events\n",
      "Processed 339000 events\n",
      "Processed 340000 events\n",
      "Processed 344000 events\n",
      "Processed 346000 events\n",
      "Processed 348000 events\n",
      "Processed 352000 events\n",
      "Processed 353000 events\n",
      "Processed 359000 events\n",
      "Processed 360000 events\n",
      "Processed 362000 events\n",
      "Processed 369000 events\n",
      "Processed 376000 events\n",
      "Processed 399000 events\n",
      "Processed 406000 events\n",
      "Processed 412000 events\n",
      "Processed 423000 events\n",
      "Processed 431000 events\n",
      "Processed 432000 events\n",
      "Processed 433000 events\n",
      "Processed 439000 events\n",
      "Processed 445000 events\n",
      "Processed 449000 events\n",
      "Processed 451000 events\n",
      "Processed 457000 events\n",
      "Processed 462000 events\n",
      "Processed 464000 events\n",
      "Processed 467000 events\n",
      "Processed 470000 events\n",
      "Processed 475000 events\n",
      "Processed 477000 events\n",
      "Processed 478000 events\n",
      "Processed 480000 events\n",
      "Processed 489000 events\n",
      "Processed 499000 events\n",
      "Processed 502000 events\n",
      "Processed 503000 events\n",
      "Processed 505000 events\n",
      "Processed 506000 events\n",
      "Processed 507000 events\n",
      "Processed 512000 events\n",
      "Processed 518000 events\n",
      "Processed 520000 events\n",
      "Processed 532000 events\n",
      "Processed 538000 events\n",
      "Processed 545000 events\n",
      "Processed 557000 events\n",
      "Processed 566000 events\n",
      "Processed 567000 events\n",
      "Processed 574000 events\n",
      "Processed 575000 events\n",
      "Processed 578000 events\n",
      "Processed 579000 events\n",
      "Processed 580000 events\n",
      "Processed 581000 events\n",
      "Processed 583000 events\n",
      "Processed 584000 events\n",
      "Processed 586000 events\n",
      "Processed 587000 events\n",
      "Processed 588000 events\n",
      "Processed 591000 events\n",
      "Processed 593000 events\n",
      "Processed 596000 events\n",
      "Processed 597000 events\n",
      "Processed 600000 events\n",
      "Processed 606000 events\n",
      "Processed 612000 events\n",
      "Processed 614000 events\n",
      "Processed 616000 events\n",
      "Processed 618000 events\n",
      "Processed 619000 events\n",
      "Processed 621000 events\n",
      "Processed 623000 events\n",
      "Processed 625000 events\n",
      "Processed 632000 events\n",
      "Processed 638000 events\n",
      "Processed 639000 events\n",
      "Processed 640000 events\n",
      "Processed 641000 events\n",
      "Processed 642000 events\n",
      "Processed 643000 events\n",
      "Processed 644000 events\n",
      "Processed 645000 events\n",
      "Processed 646000 events\n",
      "Processed 648000 events\n",
      "Processed 650000 events\n",
      "Processed 652000 events\n",
      "Processed 653000 events\n",
      "Processed 654000 events\n",
      "Processed 655000 events\n",
      "Processed 656000 events\n",
      "Processed 657000 events\n",
      "Processed 659000 events\n",
      "Processed 662000 events\n",
      "Processed 672000 events\n",
      "Processed 676000 events\n",
      "Processed 679000 events\n",
      "Processed 682000 events\n",
      "Processed 685000 events\n",
      "Processed 692000 events\n",
      "Processed 700000 events\n",
      "Processed 701000 events\n",
      "Processed 704000 events\n",
      "Processed 707000 events\n",
      "Processed 712000 events\n",
      "Processed 726000 events\n",
      "Processed 730000 events\n",
      "Processed 739000 events\n",
      "Processed 740000 events\n",
      "Processed 743000 events\n",
      "Processed 744000 events\n",
      "Processed 745000 events\n",
      "Processed 747000 events\n",
      "Processed 749000 events\n",
      "Processed 754000 events\n",
      "Processed 757000 events\n",
      "Processed 763000 events\n",
      "Processed 764000 events\n",
      "Processed 766000 events\n",
      "Processed 774000 events\n",
      "Processed 775000 events\n",
      "Processed 780000 events\n",
      "Processed 781000 events\n",
      "Processed 798000 events\n",
      "Processed 806000 events\n",
      "Processed 807000 events\n",
      "Processed 812000 events\n",
      "Processed 816000 events\n",
      "Processed 820000 events\n",
      "Processed 821000 events\n",
      "Processed 822000 events\n",
      "Processed 823000 events\n",
      "Processed 826000 events\n",
      "Processed 828000 events\n",
      "Processed 838000 events\n",
      "Processed 842000 events\n",
      "Processed 844000 events\n",
      "Processed 852000 events\n",
      "Processed 855000 events\n",
      "Processed 862000 events\n",
      "Processed 867000 events\n",
      "Processed 868000 events\n",
      "Processed 875000 events\n",
      "Processed 877000 events\n",
      "Processed 882000 events\n",
      "Processed 885000 events\n",
      "Processed 888000 events\n",
      "Processed 889000 events\n",
      "Processed 890000 events\n",
      "Processed 893000 events\n",
      "Processed 896000 events\n",
      "Processed 906000 events\n",
      "Processed 915000 events\n",
      "Processed 920000 events\n",
      "Processed 926000 events\n",
      "Processed 928000 events\n",
      "Processed 934000 events\n",
      "Processed 945000 events\n",
      "Processed 948000 events\n",
      "Processed 949000 events\n",
      "Processed 955000 events\n",
      "Processed 956000 events\n",
      "Processed 958000 events\n",
      "Processed 962000 events\n",
      "Processed 973000 events\n",
      "Processed 976000 events\n",
      "Processed 979000 events\n",
      "Processed 983000 events\n",
      "Processed 986000 events\n",
      "Processed 987000 events\n",
      "Processed 990000 events\n",
      "Processed 992000 events\n",
      "PGA computation finished.\n",
      "Historical max PGA files saved.\n",
      "Active cells: 6714 of 16200\n",
      "Calculating distances for grid cells...\n",
      "Distance calculation for grid cells completed.\n",
      "Training Random Forest...\n",
      "Random Forest training completed.\n",
      "RandomForest -> MAE: 0.165281 | RMSE: 0.317221 | R²: 0.1890\n",
      "Training XGBoost...\n",
      "XGBoost training completed.\n",
      "XGBoost -> MAE: 0.170513 | RMSE: 0.339563 | R²: 0.0708\n",
      "Training LSTM...\n",
      "Epoch 1/10\n",
      "\u001b[1m2644/2644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/10\n",
      "\u001b[1m2644/2644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/10\n",
      "\u001b[1m2644/2644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/10\n",
      "\u001b[1m2644/2644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - loss: nan - val_loss: nan\n",
      "Epoch 5/10\n",
      "\u001b[1m2644/2644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - loss: nan - val_loss: nan\n",
      "Epoch 6/10\n",
      "\u001b[1m2644/2644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - loss: nan - val_loss: nan\n",
      "Epoch 7/10\n",
      "\u001b[1m2644/2644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - loss: nan - val_loss: nan\n",
      "Epoch 8/10\n",
      "\u001b[1m2644/2644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - loss: nan - val_loss: nan\n",
      "Epoch 9/10\n",
      "\u001b[1m2644/2644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - loss: nan - val_loss: nan\n",
      "Epoch 10/10\n",
      "\u001b[1m2644/2644\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - loss: nan - val_loss: nan\n",
      "LSTM training completed.\n",
      "LSTM step skipped. Reason: Input contains NaN.\n",
      "Generating XGBoost forecasts...\n",
      "XGBoost forecasts completed.\n",
      "Generating Random Forest forecasts...\n",
      "Random Forest forecasts completed.\n",
      "\n",
      "Saved metrics to: D:/Earthquake_Project/pga_outputs2\\model_metrics_validation_2016_2025.csv\n",
      "Forecast files saved in: D:/Earthquake_Project/pga_outputs2\n",
      "Expected outputs include:\n",
      "  - PGA_RF_{2030,2050,2100}.tif / .csv\n",
      "  - PGA_XGB_{2030,2050,2100}.tif / .csv\n"
     ]
    }
   ],
   "source": [
    "import os, math, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import rasterio\n",
    "from rasterio.transform import from_origin\n",
    "import geopandas as gpd\n",
    "from shapely.strtree import STRtree\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ================== CONFIG ==================\n",
    "CATALOG_CSV       = r\"D:/Earthquake_Project/Datasets/Earthquake_datasets.csv\"\n",
    "ACTIVE_FAULTS_MARGINS_SHP = r\"D:/Earthquake_Project/Datasets/Active faults/gem_active_faults.shp\"\n",
    "OUT_DIR           = r\"D:/Earthquake_Project/pga_outputs2\"\n",
    "GRID_STEP_DEG     = 2.0  # Adjustable grid resolution\n",
    "MIN_MAG           = 4.5\n",
    "MAX_DIST_KM       = 200.0  # Adjustable max distance\n",
    "YEARS_RANGE       = (1950, 2025)\n",
    "FORECAST_YEARS    = [2030, 2050, 2100]\n",
    "LAGS              = 3  # Adjustable lag window\n",
    "SEED              = 42\n",
    "# Updated GMPE-like proxy coefficients (inspired by Boore-Atkinson 2008, adjusted)\n",
    "C0, C1, C2, H     = -3.5, 1.0, 1.5, 5.0  # More realistic starting point\n",
    "ACTIVE_CUTOFF     = 1e-6\n",
    "# ============================================\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "def haversine_km(lat1, lon1, lat2, lon2):\n",
    "    d2r = np.pi / 180.0\n",
    "    f1, f2 = lat1 * d2r, lat2 * d2r\n",
    "    dlat = (lat2 - lat1) * d2r\n",
    "    dlon = (lon2 - lon1) * d2r\n",
    "    a = np.sin(dlat / 2) ** 2 + np.cos(f1) * np.cos(f2) * np.sin(dlon / 2) ** 2\n",
    "    return 6371.0 * 2 * np.arcsin(np.sqrt(np.clip(a, 0, 1)))\n",
    "\n",
    "def pga_proxy(Mw, R_hyp_km, site_factor=1.0):  # Added site factor for flexibility\n",
    "    ln_pga = C0 + C1 * Mw - C2 * np.log10(R_hyp_km + H) + np.log10(site_factor)\n",
    "    return np.exp(ln_pga)\n",
    "\n",
    "def write_geotiff(path, arr2d, lats, lons, dtype=\"float32\"):\n",
    "    res_x = lons[1] - lons[0]\n",
    "    res_y = lats[1] - lats[0]\n",
    "    transform = from_origin(lons.min() - res_x / 2, lats.max() + res_y / 2, res_x, res_y)\n",
    "    with rasterio.open(path, \"w\", driver=\"GTiff\",\n",
    "                       height=arr2d.shape[0], width=arr2d.shape[1],\n",
    "                       count=1, dtype=dtype, crs=\"EPSG:4326\",\n",
    "                       transform=transform, compress=\"lzw\") as dst:\n",
    "        dst.write(np.flipud(arr2d.astype(dtype)), 1)\n",
    "\n",
    "def array_to_csv(path, arr2d, lats, lons, colname=\"pga_g\"):\n",
    "    LAT, LON = np.meshgrid(lats, lons, indexing=\"ij\")\n",
    "    pd.DataFrame({\"lat\": LAT.ravel(), \"lon\": LON.ravel(), colname: arr2d.ravel()}).to_csv(path, index=False)\n",
    "\n",
    "def evaluate(y_true, y_pred, name):\n",
    "    y_true = y_true.astype(np.float32); y_pred = y_pred.astype(np.float32)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"{name} -> MAE: {mae:.6f} | RMSE: {rmse:.6f} | R²: {r2:.4f}\")\n",
    "    return {\"model\": name, \"MAE\": mae, \"RMSE\": rmse, \"R2\": r2}\n",
    "\n",
    "# Load single shapefile with spatial index\n",
    "faults_margins = gpd.read_file(ACTIVE_FAULTS_MARGINS_SHP)\n",
    "spatial_index = STRtree(faults_margins.geometry)\n",
    "\n",
    "def nearest_distance_to_geometry(lat, lon, geometry_gdf, spatial_idx):\n",
    "    point = gpd.points_from_xy([lon], [lat])[0]\n",
    "    possible_matches_index = list(spatial_idx.query(point))\n",
    "    if not possible_matches_index:\n",
    "        return np.nan\n",
    "    distances = [point.distance(geom) * 111 for geom in [geometry_gdf.geometry.iloc[i] for i in possible_matches_index]]\n",
    "    return min(distances) if distances else np.nan\n",
    "\n",
    "# 1) Load catalog\n",
    "eq = pd.read_csv(CATALOG_CSV)\n",
    "eq.columns = [c.strip().lower() for c in eq.columns]\n",
    "for col in [\"time\", \"latitude\", \"longitude\", \"depth\", \"mag\"]:\n",
    "    if col not in eq.columns: raise ValueError(f\"Missing column: {col}\")\n",
    "eq[\"time\"] = pd.to_datetime(eq[\"time\"], errors=\"coerce\")\n",
    "eq = eq.dropna(subset=[\"time\", \"latitude\", \"longitude\", \"depth\", \"mag\"])\n",
    "eq = eq[(eq[\"time\"].dt.year >= YEARS_RANGE[0]) & (eq[\"time\"].dt.year <= YEARS_RANGE[1])]\n",
    "eq = eq[eq[\"mag\"] >= MIN_MAG].copy()\n",
    "eq[\"year\"] = eq[\"time\"].dt.year.astype(int)\n",
    "print(f\"Events used: {len(eq):,}\")\n",
    "\n",
    "# Add distance feature to earthquake data\n",
    "print(\"Calculating distances for earthquakes...\")\n",
    "eq[\"dist_to_fault_margin_km\"] = eq.apply(lambda row: nearest_distance_to_geometry(row[\"latitude\"], row[\"longitude\"], faults_margins, spatial_index), axis=1)\n",
    "print(\"Distance calculation for earthquakes completed.\")\n",
    "\n",
    "# 2) Grid\n",
    "lats = np.arange(-90 + GRID_STEP_DEG / 2, 90, GRID_STEP_DEG)\n",
    "lons = np.arange(-180 + GRID_STEP_DEG / 2, 180, GRID_STEP_DEG)\n",
    "nlat, nlon = len(lats), len(lons)\n",
    "years = np.arange(YEARS_RANGE[0], YEARS_RANGE[1] + 1, dtype=int)\n",
    "ny = len(years)\n",
    "year_to_idx = {y: i for i, y in enumerate(years)}\n",
    "print(f\"Grid: {nlat} x {nlon} cells\")\n",
    "\n",
    "lat_bins = np.floor(lats).astype(int)\n",
    "lon_bins = np.floor(lons).astype(int)\n",
    "bin_index = {}\n",
    "for i, la in enumerate(lat_bins):\n",
    "    for j, lo in enumerate(lon_bins):\n",
    "        bin_index.setdefault((la, lo), []).append((i, j))\n",
    "\n",
    "# 3) Annual max PGA per cell with distance features\n",
    "pga_year = np.zeros((ny, nlat, nlon), dtype=np.float32)\n",
    "ddeg = MAX_DIST_KM / 111.0\n",
    "print(\"Starting PGA computation...\")\n",
    "for idx, ev in eq.iterrows():\n",
    "    if idx % 1000 == 0:  # Progress update\n",
    "        print(f\"Processed {idx} events\")\n",
    "    y = int(ev[\"year\"]); iy = year_to_idx[y]\n",
    "    ev_lat, ev_lon = float(ev[\"latitude\"]), float(ev[\"longitude\"])\n",
    "    dep = max(0.0, float(ev[\"depth\"])); Mw = float(ev[\"mag\"])\n",
    "    lat_lo = math.floor(ev_lat - ddeg); lat_hi = math.floor(ev_lat + ddeg)\n",
    "    lon_lo = math.floor(ev_lon - ddeg); lon_hi = math.floor(ev_lon + ddeg)\n",
    "    cand = []\n",
    "    for la in range(lat_lo, lat_hi + 1):\n",
    "        for lo in range(lon_lo, lon_hi + 1):\n",
    "            if (la, lo) in bin_index:\n",
    "                cand.extend(bin_index[(la, lo)])\n",
    "    if not cand: continue\n",
    "    ci, cj = zip(*cand)\n",
    "    ci = np.array(ci); cj = np.array(cj)\n",
    "    dist = haversine_km(ev_lat, ev_lon, lats[ci], lons[cj])\n",
    "    mask = dist <= MAX_DIST_KM\n",
    "    if not np.any(mask): continue\n",
    "    ci = ci[mask]; cj = cj[mask]\n",
    "    R_hyp = np.sqrt(dist[mask] ** 2 + dep ** 2)\n",
    "    # Simple site factor based on distance to fault/margin (adjustable)\n",
    "    site_factor = np.where(dist[mask] < 50.0, 1.2, 1.0)  # Boost near faults\n",
    "    pga_v = pga_proxy(Mw, R_hyp, site_factor).astype(np.float32)\n",
    "    pga_year[iy, ci, cj] = np.maximum(pga_year[iy, ci, cj], pga_v)\n",
    "print(\"PGA computation finished.\")\n",
    "\n",
    "# Historical base & active mask\n",
    "hist_max = np.nanmax(pga_year, axis=0).astype(np.float32)\n",
    "write_geotiff(os.path.join(OUT_DIR, \"PGA_hist_max_1950_2025.tif\"), hist_max, lats, lons)\n",
    "array_to_csv(os.path.join(OUT_DIR, \"PGA_hist_max_1950_2025.csv\"), hist_max, lats, lons)\n",
    "print(\"Historical max PGA files saved.\")\n",
    "\n",
    "active_mask = hist_max > ACTIVE_CUTOFF\n",
    "print(\"Active cells:\", int(active_mask.sum()), \"of\", nlat * nlon)\n",
    "\n",
    "# 4) Long format + lags (active cells only) with distance features\n",
    "rows = []\n",
    "LAT, LON = np.meshgrid(lats, lons, indexing=\"ij\")\n",
    "print(\"Calculating distances for grid cells...\")\n",
    "dist_fault_margin = np.array([[nearest_distance_to_geometry(lat, lon, faults_margins, spatial_index) for lon in lons] for lat in lats])\n",
    "print(\"Distance calculation for grid cells completed.\")\n",
    "for yi, y in enumerate(years):\n",
    "    arr = pga_year[yi]\n",
    "    arr = np.where(active_mask, arr, np.nan)\n",
    "    lat_grid, lon_grid = LAT.ravel(), LON.ravel()\n",
    "    dist_fault_margin_flat = dist_fault_margin.ravel()\n",
    "    rows.append(pd.DataFrame({\n",
    "        \"year\": y,\n",
    "        \"lat\": lat_grid,\n",
    "        \"lon\": lon_grid,\n",
    "        \"pga_g\": arr.ravel(),\n",
    "        \"dist_to_fault_margin_km\": dist_fault_margin_flat\n",
    "    }))\n",
    "long_df = pd.concat(rows, ignore_index=True).dropna(subset=[\"pga_g\"])\n",
    "\n",
    "long_df = long_df.sort_values([\"lat\", \"lon\", \"year\"])\n",
    "for k in range(1, LAGS + 1):\n",
    "    long_df[f\"pga_lag{k}\"] = long_df.groupby([\"lat\", \"lon\"])[\"pga_g\"].shift(k)\n",
    "ml_df = long_df.dropna().reset_index(drop=True)\n",
    "\n",
    "# 5) Time-aware split\n",
    "train = ml_df[ml_df[\"year\"] <= 2015].copy()\n",
    "valid = ml_df[(ml_df[\"year\"] > 2015) & (ml_df[\"year\"] <= 2025)].copy()\n",
    "\n",
    "FEATURES = [f\"pga_lag{k}\" for k in range(1, LAGS + 1)] + [\"lat\", \"lon\", \"year\", \"dist_to_fault_margin_km\"]\n",
    "TARGET = \"pga_g\"\n",
    "\n",
    "# Downcast to float32\n",
    "X_tr = train[FEATURES].astype(np.float32).values\n",
    "y_tr = train[TARGET].astype(np.float32).values\n",
    "X_va = valid[FEATURES].astype(np.float32).values\n",
    "y_va = valid[TARGET].astype(np.float32).values\n",
    "\n",
    "metrics_rows = []\n",
    "\n",
    "# 6) Random Forest\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=100,  # Reduced for speed\n",
    "    max_depth=10,      # Reduced to prevent overfitting\n",
    "    max_features=0.5,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    bootstrap=True,\n",
    "    max_samples=0.5,\n",
    "    random_state=SEED,\n",
    "    n_jobs=1\n",
    ")\n",
    "print(\"Training Random Forest...\")\n",
    "rf.fit(X_tr, y_tr)\n",
    "print(\"Random Forest training completed.\")\n",
    "pred_va_rf = rf.predict(X_va).astype(np.float32)\n",
    "metrics_rows.append(evaluate(y_va, pred_va_rf, \"RandomForest\"))\n",
    "\n",
    "# 7) XGBoost\n",
    "xgb_available = True\n",
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "    xgb = XGBRegressor(\n",
    "        n_estimators=200,  # Reduced for speed\n",
    "        learning_rate=0.1,\n",
    "        max_depth=6,       # Reduced to prevent overfitting\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_lambda=1.0,\n",
    "        tree_method=\"hist\",\n",
    "        max_bin=64,\n",
    "        random_state=SEED,\n",
    "        objective=\"reg:squarederror\",\n",
    "        n_jobs=1\n",
    "    )\n",
    "    print(\"Training XGBoost...\")\n",
    "    xgb.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], verbose=False)\n",
    "    print(\"XGBoost training completed.\")\n",
    "    pred_va_xgb = xgb.predict(X_va).astype(np.float32)\n",
    "    metrics_rows.append(evaluate(y_va, pred_va_xgb, \"XGBoost\"))\n",
    "except Exception as e:\n",
    "    xgb_available = False\n",
    "    print(\"XGBoost not available, skipping. Reason:\", e)\n",
    "\n",
    "# 8) LSTM\n",
    "lstm_available = True\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras import layers, models\n",
    "\n",
    "    seqs_X_tr, seqs_y_tr = [], []\n",
    "    seqs_X_va, seqs_y_va = [], []\n",
    "    for (lat, lon), grp in long_df.groupby([\"lat\", \"lon\"]):\n",
    "        g = grp.sort_values(\"year\")\n",
    "        vals = g[\"pga_g\"].values.astype(\"float32\")\n",
    "        yrs = g[\"year\"].values\n",
    "        dist_fault_margin = g[\"dist_to_fault_margin_km\"].iloc[0]\n",
    "        if len(vals) < (LAGS + 1): continue\n",
    "        for t in range(LAGS, len(vals)):\n",
    "            seq = vals[t - LAGS:t]\n",
    "            target = vals[t]\n",
    "            feat_seq = np.column_stack([seq, [dist_fault_margin] * LAGS])\n",
    "            if yrs[t] <= 2015:\n",
    "                seqs_X_tr.append(feat_seq); seqs_y_tr.append(target)\n",
    "            elif 2016 <= yrs[t] <= 2025:\n",
    "                seqs_X_va.append(feat_seq); seqs_y_va.append(target)\n",
    "\n",
    "    if len(seqs_X_tr) and len(seqs_X_va):\n",
    "        X_tr_seq = np.array(seqs_X_tr, dtype=np.float32)\n",
    "        y_tr_seq = np.array(seqs_y_tr, dtype=np.float32)\n",
    "        X_va_seq = np.array(seqs_X_va, dtype=np.float32)\n",
    "        y_va_seq = np.array(seqs_y_va, dtype=np.float32)\n",
    "\n",
    "        model = models.Sequential([\n",
    "            layers.Input(shape=(LAGS, 2)),\n",
    "            layers.LSTM(32, return_sequences=False),  # Reduced units\n",
    "            layers.Dropout(0.2),  # Added to prevent overfitting\n",
    "            layers.Dense(1)\n",
    "        ])\n",
    "        model.compile(optimizer=\"adam\", loss=\"mae\")\n",
    "        print(\"Training LSTM...\")\n",
    "        model.fit(X_tr_seq, y_tr_seq, epochs=10, batch_size=128, validation_split=0.2, verbose=1)\n",
    "        print(\"LSTM training completed.\")\n",
    "        lstm_pred_va = model.predict(X_va_seq, verbose=0).ravel().astype(np.float32)\n",
    "        metrics_rows.append(evaluate(y_va_seq, lstm_pred_va, \"LSTM\"))\n",
    "\n",
    "        # Rolling forecast for LSTM\n",
    "        lstm_state = {}\n",
    "        for (lat, lon), grp in long_df.groupby([\"lat\", \"lon\"]):\n",
    "            g = grp.sort_values(\"year\")\n",
    "            vals = g[g[\"year\"] <= 2025][\"pga_g\"].values.astype(\"float32\")\n",
    "            dist_fault_margin = g[\"dist_to_fault_margin_km\"].iloc[0]\n",
    "            if len(vals) >= LAGS:\n",
    "                lstm_state[(lat, lon)] = list(zip(vals[-LAGS:], [dist_fault_margin] * LAGS))\n",
    "\n",
    "        lstm_fc_grids = {yr: np.full((len(lats), len(lons)), np.nan, dtype=np.float32) for yr in FORECAST_YEARS}\n",
    "        for yr in sorted(FORECAST_YEARS):\n",
    "            for step in range(2026, yr + 1):\n",
    "                for (lat, lon), tail in lstm_state.items():\n",
    "                    if len(tail) < LAGS: continue\n",
    "                    x = np.array([t[0] for t in tail], dtype=np.float32)[None, :, None]\n",
    "                    yhat = float(model.predict(x, verbose=0)[0, 0])\n",
    "                    tail.append((yhat, tail[0][1]))\n",
    "                    if len(tail) > LAGS: tail.pop(0)\n",
    "            for (lat, lon), tail in lstm_state.items():\n",
    "                i = np.where(lats == lat)[0]; j = np.where(lons == lon)[0]\n",
    "                if i.size and j.size:\n",
    "                    lstm_fc_grids[yr][i[0], j[0]] = tail[-1][0]\n",
    "\n",
    "        for yr, grid in lstm_fc_grids.items():\n",
    "            write_geotiff(os.path.join(OUT_DIR, f\"PGA_LSTM_{yr}.tif\"), grid, lats, lons)\n",
    "            array_to_csv(os.path.join(OUT_DIR, f\"PGA_LSTM_{yr}.csv\"), grid, lats, lons)\n",
    "    else:\n",
    "        print(\"Not enough sequences for LSTM; skipping.\")\n",
    "except Exception as e:\n",
    "    lstm_available = False\n",
    "    print(\"LSTM step skipped. Reason:\", e)\n",
    "\n",
    "# 9) Rolling forecast for RF and XGB\n",
    "def roll_forecast(model, start_year=2025, target_years=FORECAST_YEARS):\n",
    "    last = long_df[long_df[\"year\"] <= start_year].sort_values([\"lat\", \"lon\", \"year\"])\n",
    "    buff = last.groupby([\"lat\", \"lon\"]).apply(lambda g: list(zip(g[\"pga_g\"].tail(LAGS), g[\"dist_to_fault_margin_km\"].head(1)))).to_dict()\n",
    "    preds = {}\n",
    "    for yr in sorted(target_years):\n",
    "        for y in range(start_year + 1, yr + 1):\n",
    "            feats_rows, idx_rows = [], []\n",
    "            for (lat, lon), tail in buff.items():\n",
    "                if len(tail) < LAGS: continue\n",
    "                row = {f\"pga_lag{k}\": tail[-k][0] for k in range(1, LAGS + 1)}\n",
    "                row.update({\"lat\": lat, \"lon\": lon, \"year\": y, \"dist_to_fault_margin_km\": tail[0][1]})\n",
    "                feats_rows.append(row); idx_rows.append((lat, lon))\n",
    "            if not feats_rows: break\n",
    "            F = pd.DataFrame(feats_rows)[FEATURES].astype(np.float32).values\n",
    "            yhat = model.predict(F).astype(np.float32)\n",
    "            for (lat, lon), val in zip(idx_rows, yhat):\n",
    "                tail = buff[(lat, lon)]\n",
    "                tail.append((float(val), tail[0][1]))\n",
    "                if len(tail) > LAGS: tail.pop(0)\n",
    "        grid = np.full((len(lats), len(lons)), np.nan, dtype=np.float32)\n",
    "        for (lat, lon), tail in buff.items():\n",
    "            i = np.where(lats == lat)[0]; j = np.where(lons == lon)[0]\n",
    "            if i.size and j.size: grid[i[0], j[0]] = tail[-1][0]\n",
    "        preds[yr] = grid\n",
    "    return preds\n",
    "\n",
    "if xgb_available:\n",
    "    print(\"Generating XGBoost forecasts...\")\n",
    "    xgb_fc = roll_forecast(xgb)\n",
    "    for yr, grid in xgb_fc.items():\n",
    "        write_geotiff(os.path.join(OUT_DIR, f\"PGA_XGB_{yr}.tif\"), grid, lats, lons)\n",
    "        array_to_csv(os.path.join(OUT_DIR, f\"PGA_XGB_{yr}.csv\"), grid, lats, lons)\n",
    "    print(\"XGBoost forecasts completed.\")\n",
    "\n",
    "rf_fc = roll_forecast(rf)\n",
    "print(\"Generating Random Forest forecasts...\")\n",
    "for yr, grid in rf_fc.items():\n",
    "    write_geotiff(os.path.join(OUT_DIR, f\"PGA_RF_{yr}.tif\"), grid, lats, lons)\n",
    "    array_to_csv(os.path.join(OUT_DIR, f\"PGA_RF_{yr}.csv\"), grid, lats, lons)\n",
    "print(\"Random Forest forecasts completed.\")\n",
    "\n",
    "# 10) Save metrics\n",
    "metrics_df = pd.DataFrame(metrics_rows)\n",
    "metrics_path = os.path.join(OUT_DIR, \"model_metrics_validation_2016_2025.csv\")\n",
    "metrics_df.to_csv(metrics_path, index=False)\n",
    "print(\"\\nSaved metrics to:\", metrics_path)\n",
    "print(\"Forecast files saved in:\", OUT_DIR)\n",
    "print(\"Expected outputs include:\")\n",
    "print(\"  - PGA_RF_{2030,2050,2100}.tif / .csv\")\n",
    "if xgb_available:\n",
    "    print(\"  - PGA_XGB_{2030,2050,2100}.tif / .csv\")\n",
    "if lstm_available:\n",
    "    print(\"  - PGA_LSTM_{2030,2050,2100}.tif / .csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ccd47ba-c486-441e-9074-7d76a4b03f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data ...\n",
      "Catalogue: 271,987 events | Faults: 16,195 features\n",
      "Building global grid ...\n",
      "Computing features (this can take time globally) ...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 445\u001b[0m\n\u001b[0;32m    441\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 445\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[12], line 417\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    414\u001b[0m grid \u001b[38;5;241m=\u001b[39m make_global_grid(cfg\u001b[38;5;241m.\u001b[39mGRID_RES_DEG)\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComputing features (this can take time globally) ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 417\u001b[0m feat \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_features_for_grid\u001b[49m\u001b[43m(\u001b[49m\u001b[43meq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfaults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;66;03m# Historical labels for ML correction (optional but recommended)\u001b[39;00m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating historical labels for ML correction ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[12], line 256\u001b[0m, in \u001b[0;36mbuild_features_for_grid\u001b[1;34m(eq, faults, grid, cfg)\u001b[0m\n\u001b[0;32m    249\u001b[0m     fden \u001b[38;5;241m=\u001b[39m fault_density_km_per_area(poly, faults, cfg\u001b[38;5;241m.\u001b[39mR_FAULT_KM)\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlat\u001b[39m\u001b[38;5;124m\"\u001b[39m: lat0, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlon\u001b[39m\u001b[38;5;124m\"\u001b[39m: lon0, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrate_py\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(rate_py), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(b) \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39misfinite(b) \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39mnan,\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmmax_hist\u001b[39m\u001b[38;5;124m\"\u001b[39m: mmax_hist, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mm95\u001b[39m\u001b[38;5;124m\"\u001b[39m: m95, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdist_fault_km\u001b[39m\u001b[38;5;124m\"\u001b[39m: dist_fault_km, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfault_density\u001b[39m\u001b[38;5;124m\"\u001b[39m: fden,\n\u001b[0;32m    254\u001b[0m     }\n\u001b[1;32m--> 256\u001b[0m rows \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCFG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mN_JOBS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_cell\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    257\u001b[0m feat \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(rows)\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m feat\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\joblib\\parallel.py:2072\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2066\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2067\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2068\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2069\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2070\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\joblib\\parallel.py:1682\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1679\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1681\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1682\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1684\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1685\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1686\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1687\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1688\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\joblib\\parallel.py:1800\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1789\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_ordered:\n\u001b[0;32m   1790\u001b[0m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[0;32m   1791\u001b[0m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1795\u001b[0m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[0;32m   1796\u001b[0m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[0;32m   1797\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   1798\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING\n\u001b[0;32m   1799\u001b[0m     ):\n\u001b[1;32m-> 1800\u001b[0m         \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1801\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1803\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1804\u001b[0m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[0;32m   1805\u001b[0m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1811\u001b[0m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[0;32m   1812\u001b[0m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Global PGA (Peak Ground Acceleration) Forecasts for 2030, 2050, 2100\n",
    "====================================================================\n",
    "\n",
    "Inputs (already uploaded by user):\n",
    "- /mnt/data/Earthquake.csv               : global earthquake catalogue\n",
    "- /mnt/data/gem_active_faults.shp (+.*) : global active fault lines (GEM)\n",
    "\n",
    "Outputs:\n",
    "- /mnt/data/pga_forecast_2030.tif, _2050.tif, _2100.tif (GeoTIFF rasters in g units)\n",
    "- /mnt/data/pga_forecast_{year}.csv     (grid cell centroids + PGA)\n",
    "- /mnt/data/pga_features_{year}.parquet (features used for ML)\n",
    "\n",
    "Notes\n",
    "-----\n",
    "1) This script provides a *complete, reproducible pipeline* you can run locally.\n",
    "   It includes:\n",
    "   - Data loading & cleaning\n",
    "   - Global equal-angle grid generation\n",
    "   - Spatial features: seismicity rate, b-value, max magnitude, nearest-fault distance, fault density\n",
    "   - Physics-inspired baseline (Gutenberg–Richter + simple attenuation) to derive a prior PGA\n",
    "   - ML model (GradientBoostingRegressor) that learns residual corrections\n",
    "   - Forecasts for horizons H = 5y (to 2030), 25y (to 2050), 75y (to 2100) from a reference year\n",
    "\n",
    "2) GMPE: By default we use a *simple magnitude–distance attenuation proxy* to turn an\n",
    "   extreme-event magnitude (per cell & horizon) into a PGA prior. Coefficients are tunable.\n",
    "   If you have OpenQuake Hazardlib installed, you can plug in a proper GMPE (see the\n",
    "   TODO marked section), which is recommended for production.\n",
    "\n",
    "3) Performance: global computations can be heavy. Start with a coarser grid (e.g., 1°) and\n",
    "   smaller search radius, then refine.\n",
    "\n",
    "4) Column names: set them in the CONFIG section to match your CSV.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import os\n",
    "import math\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, box\n",
    "from shapely.ops import unary_union\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Optional (only used if raster export is desired and rasterio is available)\n",
    "try:\n",
    "    import rasterio\n",
    "    from rasterio.transform import from_origin\n",
    "except Exception:  # pragma: no cover\n",
    "    rasterio = None\n",
    "    from_origin = None\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# ----------------------------- CONFIG ---------------------------------\n",
    "@dataclass\n",
    "class Config:\n",
    "    EQ_CSV: str = r\"D:/Earthquake_Project/Datasets/Earthquake_datasets.csv\"\n",
    "    FAULTS_SHP: str = r\"D:/Earthquake_Project/Datasets/Active faults/gem_active_faults.shp\"\n",
    "\n",
    "    # Map your earthquake CSV columns here\n",
    "    LAT_COL: str = \"latitude\"   # alternatives: lat, Latitude\n",
    "    LON_COL: str = \"longitude\"  # alternatives: lon, Longitude\n",
    "    MAG_COL: str = \"mag\"         # magnitude (Mw preferred)\n",
    "    DEP_COL: str = \"depth\"       # in km (if missing, set a constant)\n",
    "    TIME_COL: str = \"time\"       # datetime string\n",
    "\n",
    "    # Catalogue filtering\n",
    "    YEAR_MIN: int = 1970\n",
    "    YEAR_MAX: int = 2024\n",
    "    MMIN: float = 4.5             # completeness magnitude threshold\n",
    "\n",
    "    # Global grid\n",
    "    GRID_RES_DEG: float = 1.0     # 1° grid to start; later try 0.5° or 0.25°\n",
    "\n",
    "    # Feature radii (km)\n",
    "    R_SEIS_KM: float = 500.0      # radius to gather earthquakes for local stats\n",
    "    R_FAULT_KM: float = 200.0     # radius for fault density\n",
    "    SEARCH_MAX_KM: float = 300.0  # max distance for event-to-cell PGA calculation (labels)\n",
    "\n",
    "    # Horizons (years ahead from REF_YEAR)\n",
    "    REF_YEAR: int = 2025          # today ~ 2025-08; adjust as needed\n",
    "    HORIZONS: Tuple[int, ...] = (5, 25, 75)  # -> 2030, 2050, 2100\n",
    "\n",
    "    # Simple attenuation proxy coefficients (log10 PGA in g)\n",
    "    # log10(PGA_g) = A + B*M - C*log10(Rhyp + R0) - D*Rhyp_km\n",
    "    A: float = -1.20\n",
    "    B: float = 0.30\n",
    "    C: float = 1.10\n",
    "    D: float = 0.0005\n",
    "    R0: float = 10.0\n",
    "    DEP_FLOOR_KM: float = 5.0\n",
    "\n",
    "    N_JOBS: int = max(1, os.cpu_count() - 1)\n",
    "    RANDOM_STATE: int = 42\n",
    "\n",
    "CFG = Config()\n",
    "\n",
    "# ------------------------- HELPER FUNCTIONS ----------------------------\n",
    "EARTH_RADIUS_KM = 6371.0088\n",
    "\n",
    "\n",
    "def haversine_km(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Great-circle distance in km between two points (lat/lon in degrees).\"\"\"\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2)**2\n",
    "    c = 2*np.arcsin(np.sqrt(a))\n",
    "    return EARTH_RADIUS_KM * c\n",
    "\n",
    "\n",
    "def rhyp_km(r_epi_km: np.ndarray, depth_km: np.ndarray) -> np.ndarray:\n",
    "    return np.sqrt(r_epi_km**2 + np.maximum(depth_km, CFG.DEP_FLOOR_KM)**2)\n",
    "\n",
    "\n",
    "def simple_pga_proxy_g(M: np.ndarray, Rhyp_km: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"A tunable, simple magnitude–distance attenuation for PGA (in g).\n",
    "    This is a *placeholder*. For production, replace with a GMPE from OpenQuake.\n",
    "    \"\"\"\n",
    "    log10_pga = CFG.A + CFG.B*M - CFG.C*np.log10(Rhyp_km + CFG.R0) - CFG.D*Rhyp_km\n",
    "    return np.maximum(10**log10_pga, 1e-6)  # clamp tiny values\n",
    "\n",
    "\n",
    "def year_fraction(dt: pd.Series) -> pd.Series:\n",
    "    return dt.dt.year + (dt.dt.dayofyear - 1)/365.25\n",
    "\n",
    "\n",
    "def gr_b_value(mags: np.ndarray, mmin: float) -> float:\n",
    "    \"\"\"Compute b-value by Aki (1965) MLE; returns np.nan if not enough data.\"\"\"\n",
    "    mags = mags[~np.isnan(mags)]\n",
    "    if mags.size < 20:\n",
    "        return np.nan\n",
    "    mean_m = mags.mean()\n",
    "    return (np.log10(np.e)) / (mean_m - mmin + 1e-6)\n",
    "\n",
    "\n",
    "def make_global_grid(res_deg: float) -> gpd.GeoDataFrame:\n",
    "    lats = np.arange(-90 + res_deg/2, 90, res_deg)\n",
    "    lons = np.arange(-180 + res_deg/2, 180, res_deg)\n",
    "    rows = []\n",
    "    for lat in lats:\n",
    "        for lon in lons:\n",
    "            cell = box(lon - res_deg/2, lat - res_deg/2, lon + res_deg/2, lat + res_deg/2)\n",
    "            rows.append({\"lat\": float(lat), \"lon\": float(lon), \"geometry\": cell})\n",
    "    gdf = gpd.GeoDataFrame(rows, crs=\"EPSG:4326\")\n",
    "    return gdf\n",
    "\n",
    "\n",
    "def points_within_radius_idx(lat0, lon0, lats, lons, radius_km) -> np.ndarray:\n",
    "    d = haversine_km(lat0, lon0, lats, lons)\n",
    "    return np.where(d <= radius_km)[0]\n",
    "\n",
    "\n",
    "def fault_density_km_per_area(cell_poly, faults_gdf, buffer_km=CFG.R_FAULT_KM) -> float:\n",
    "    # approximate by projecting locally to EPSG:3857 for length\n",
    "    local = gpd.GeoDataFrame(geometry=[cell_poly], crs=\"EPSG:4326\").to_crs(3857)\n",
    "    faults_3857 = faults_gdf.to_crs(3857)\n",
    "    buf = local.buffer(buffer_km*1000).iloc[0]\n",
    "    sel = faults_3857[faults_3857.intersects(buf)]\n",
    "    if sel.empty:\n",
    "        return 0.0\n",
    "    total_len_m = sel.length.sum()\n",
    "    area_km2 = local.area.iloc[0] / 1e6\n",
    "    return float((total_len_m / 1000.0) / max(area_km2, 1e-6))\n",
    "\n",
    "\n",
    "# --------------------------- DATA LOADING ------------------------------\n",
    "\n",
    "def load_data(cfg: Config):\n",
    "    eq = pd.read_csv(cfg.EQ_CSV, low_memory=False)\n",
    "    # Standardize column names\n",
    "    cols = {cfg.LAT_COL: \"lat\", cfg.LON_COL: \"lon\", cfg.MAG_COL: \"mag\"}\n",
    "    if cfg.DEP_COL in eq.columns:\n",
    "        cols[cfg.DEP_COL] = \"depth\"\n",
    "    if cfg.TIME_COL in eq.columns:\n",
    "        cols[cfg.TIME_COL] = \"time\"\n",
    "    eq = eq.rename(columns=cols)\n",
    "\n",
    "    # Parse time\n",
    "    if \"time\" in eq.columns:\n",
    "        eq[\"time\"] = pd.to_datetime(eq[\"time\"], errors=\"coerce\")\n",
    "    else:\n",
    "        eq[\"time\"] = pd.NaT\n",
    "\n",
    "    # Filter\n",
    "    if eq[\"time\"].notna().any():\n",
    "        eq = eq[(eq[\"time\"].dt.year >= cfg.YEAR_MIN) & (eq[\"time\"].dt.year <= cfg.YEAR_MAX)]\n",
    "    eq = eq[eq[\"mag\"] >= cfg.MMIN].copy()\n",
    "    eq[\"depth\"] = eq.get(\"depth\", pd.Series(np.full(len(eq), 10.0)))\n",
    "    eq = eq.dropna(subset=[\"lat\",\"lon\",\"mag\"]).reset_index(drop=True)\n",
    "\n",
    "    faults = gpd.read_file(cfg.FAULTS_SHP)\n",
    "    if faults.crs is None:\n",
    "        faults.set_crs(\"EPSG:4326\", inplace=True)\n",
    "    else:\n",
    "        faults = faults.to_crs(\"EPSG:4326\")\n",
    "\n",
    "    return eq, faults\n",
    "\n",
    "\n",
    "# ----------------------- FEATURE ENGINEERING --------------------------\n",
    "\n",
    "def build_features_for_grid(eq: pd.DataFrame, faults: gpd.GeoDataFrame, grid: gpd.GeoDataFrame, cfg: Config) -> pd.DataFrame:\n",
    "    # Pre-arrays for speed\n",
    "    eq_lat = eq[\"lat\"].to_numpy()\n",
    "    eq_lon = eq[\"lon\"].to_numpy()\n",
    "    eq_mag = eq[\"mag\"].to_numpy()\n",
    "    eq_dep = eq[\"depth\"].to_numpy()\n",
    "    eq_year = np.where(eq[\"time\"].notna(), eq[\"time\"].dt.year.to_numpy(), np.full(len(eq), np.nan))\n",
    "\n",
    "    T_years = np.nanmax(eq_year) - np.nanmin(eq_year) if np.isfinite(eq_year).all() else (cfg.YEAR_MAX - cfg.YEAR_MIN + 1)\n",
    "    T_years = max(T_years, 1)\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    def process_cell(row):\n",
    "        lat0 = row[\"lat\"]; lon0 = row[\"lon\"]; poly = row[\"geometry\"]\n",
    "        idx = points_within_radius_idx(lat0, lon0, eq_lat, eq_lon, cfg.R_SEIS_KM)\n",
    "        n = idx.size\n",
    "        if n == 0:\n",
    "            fden = fault_density_km_per_area(poly, faults, cfg.R_FAULT_KM)\n",
    "            return {\n",
    "                \"lat\": lat0, \"lon\": lon0, \"rate_py\": 0.0, \"b\": np.nan, \"mmax_hist\": np.nan,\n",
    "                \"m95\": np.nan, \"dist_fault_km\": np.nan, \"fault_density\": fden,\n",
    "            }\n",
    "        mags = eq_mag[idx]\n",
    "        b = gr_b_value(mags, cfg.MMIN)\n",
    "        rate_py = n / T_years\n",
    "        mmax_hist = float(np.nanmax(mags)) if mags.size else np.nan\n",
    "        # 95th percentile magnitude as robust high-tail\n",
    "        m95 = float(np.nanpercentile(mags, 95)) if mags.size else np.nan\n",
    "\n",
    "        # Nearest fault distance (km) using centroid\n",
    "        centroid = gpd.GeoSeries([poly.centroid], crs=\"EPSG:4326\").to_crs(3857)\n",
    "        faults3857 = faults.to_crs(3857)\n",
    "        dmin_m = faults3857.distance(centroid.iloc[0]).min() if len(faults3857) else np.nan\n",
    "        dist_fault_km = float(dmin_m / 1000.0) if np.isfinite(dmin_m) else np.nan\n",
    "\n",
    "        # Fault density around cell\n",
    "        fden = fault_density_km_per_area(poly, faults, cfg.R_FAULT_KM)\n",
    "\n",
    "        return {\n",
    "            \"lat\": lat0, \"lon\": lon0, \"rate_py\": float(rate_py), \"b\": float(b) if np.isfinite(b) else np.nan,\n",
    "            \"mmax_hist\": mmax_hist, \"m95\": m95, \"dist_fault_km\": dist_fault_km, \"fault_density\": fden,\n",
    "        }\n",
    "\n",
    "    rows = Parallel(n_jobs=CFG.N_JOBS, prefer=\"threads\")(delayed(process_cell)(grid.iloc[i]) for i in range(len(grid)))\n",
    "    feat = pd.DataFrame(rows)\n",
    "    return feat\n",
    "\n",
    "\n",
    "# ---------------------- EXTREME MAGNITUDE MODEL -----------------------\n",
    "\n",
    "def expected_extreme_mag(rate_py: float, b: float, horizon_y: int, mmin: float) -> float:\n",
    "    \"\"\"Simple GR-based extreme magnitude in horizon H years.\n",
    "    If b is missing, fall back to mmin + log10(N) with b=1.\n",
    "    N = rate_py * H\n",
    "    Mmax ~ mmin + (1/b)*log10(max(N, 1))\n",
    "    \"\"\"\n",
    "    N = max(rate_py * horizon_y, 1e-6)\n",
    "    if not np.isfinite(b) or b <= 0:\n",
    "        b = 1.0\n",
    "    return mmin + (1.0 / b) * math.log10(max(N, 1.0))\n",
    "\n",
    "\n",
    "def pga_prior_from_features(feat_row: pd.Series, horizon_y: int, cfg: Config) -> float:\n",
    "    Mext = max(expected_extreme_mag(feat_row[\"rate_py\"], feat_row[\"b\"], horizon_y, cfg.MMIN), feat_row.get(\"m95\", cfg.MMIN))\n",
    "    # Use distance to nearest fault as a proxy for expected source distance\n",
    "    R_epi = max(feat_row.get(\"dist_fault_km\", 50.0), 10.0)\n",
    "    Rh = math.sqrt(R_epi**2 + cfg.DEP_FLOOR_KM**2)\n",
    "    pga_g = simple_pga_proxy_g(np.array([Mext]), np.array([Rh]))[0]\n",
    "    # Scale slightly by fault density (more faults -> higher shaking potential)\n",
    "    fden = feat_row.get(\"fault_density\", 0.0)\n",
    "    scale = 1.0 + 0.05 * math.tanh(fden / 10.0)\n",
    "    return float(pga_g * scale)\n",
    "\n",
    "\n",
    "# --------------------------- ML CORRECTION -----------------------------\n",
    "\n",
    "def make_training_labels(eq: pd.DataFrame, grid: gpd.GeoDataFrame, cfg: Config, label_window_y: int = 10) -> pd.DataFrame:\n",
    "    \"\"\"Create historical labels: for reference years in the past, compute the *observed* max\n",
    "    (proxy) PGA in the next `label_window_y` using events within SEARCH_MAX_KM.\n",
    "    This lets the ML model learn corrections to the prior.\n",
    "    \"\"\"\n",
    "    if \"time\" not in eq.columns or eq[\"time\"].isna().all():\n",
    "        raise ValueError(\"Earthquake CSV lacks a usable time column; cannot create labels.\")\n",
    "\n",
    "    years = np.arange(max(CFG.YEAR_MIN, 1980), CFG.YEAR_MAX - label_window_y, label_window_y)\n",
    "\n",
    "    records = []\n",
    "    # Pre arrays\n",
    "    eq_lat = eq[\"lat\"].to_numpy(); eq_lon = eq[\"lon\"].to_numpy()\n",
    "    eq_mag = eq[\"mag\"].to_numpy(); eq_dep = eq[\"depth\"].to_numpy()\n",
    "    eq_year = eq[\"time\"].dt.year.to_numpy()\n",
    "\n",
    "    for y0 in years:\n",
    "        y1 = y0 + label_window_y\n",
    "        mask = (eq_year >= y0) & (eq_year < y1)\n",
    "        lat_sub = eq_lat[mask]; lon_sub = eq_lon[mask]\n",
    "        mag_sub = eq_mag[mask]; dep_sub = eq_dep[mask]\n",
    "        if lat_sub.size == 0:\n",
    "            continue\n",
    "\n",
    "        def label_cell(row):\n",
    "            lat0, lon0 = row[\"lat\"], row[\"lon\"]\n",
    "            d_epi = haversine_km(lat0, lon0, lat_sub, lon_sub)\n",
    "            sel = d_epi <= cfg.SEARCH_MAX_KM\n",
    "            if not np.any(sel):\n",
    "                return 1e-6\n",
    "            Rh = rhyp_km(d_epi[sel], dep_sub[sel])\n",
    "            pga = simple_pga_proxy_g(mag_sub[sel], Rh)\n",
    "            return float(np.max(pga))\n",
    "\n",
    "        labels = Parallel(n_jobs=CFG.N_JOBS, prefer=\"threads\")(delayed(label_cell)(grid.iloc[i]) for i in range(len(grid)))\n",
    "        df = pd.DataFrame({\"year0\": y0, \"year1\": y1, \"lat\": grid[\"lat\"].values, \"lon\": grid[\"lon\"].values, \"pga_obs\": labels})\n",
    "        records.append(df)\n",
    "\n",
    "    return pd.concat(records, ignore_index=True) if records else pd.DataFrame()\n",
    "\n",
    "\n",
    "def train_ml_correction(features: pd.DataFrame, labels: pd.DataFrame) -> GradientBoostingRegressor:\n",
    "    df = features.merge(labels, on=[\"lat\",\"lon\"], how=\"inner\")\n",
    "    if df.empty:\n",
    "        raise RuntimeError(\"No training data could be constructed. Check time ranges and columns.\")\n",
    "    X = df[[\"rate_py\",\"b\",\"mmax_hist\",\"m95\",\"dist_fault_km\",\"fault_density\"]].fillna(0.0)\n",
    "    y = np.log1p(df[\"pga_obs\"])  # stabilize\n",
    "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, random_state=CFG.RANDOM_STATE)\n",
    "    model = GradientBoostingRegressor(random_state=CFG.RANDOM_STATE)\n",
    "    model.fit(Xtr, ytr)\n",
    "    yhat = model.predict(Xte)\n",
    "    print(\"ML correction R2:\", r2_score(yte, yhat))\n",
    "    print(\"ML correction MAE (log1p):\", mean_absolute_error(yte, yhat))\n",
    "    return model\n",
    "\n",
    "\n",
    "# --------------------------- FORECASTING -------------------------------\n",
    "\n",
    "def forecast_pga(features: pd.DataFrame, model: GradientBoostingRegressor | None, horizon_y: int, cfg: Config) -> pd.DataFrame:\n",
    "    # Prior from physics-inspired model\n",
    "    prior = features.apply(lambda r: pga_prior_from_features(r, horizon_y, cfg), axis=1)\n",
    "    out = features[[\"lat\",\"lon\"]].copy()\n",
    "    out[\"pga_prior\"] = prior.values\n",
    "\n",
    "    if model is not None:\n",
    "        X = features[[\"rate_py\",\"b\",\"mmax_hist\",\"m95\",\"dist_fault_km\",\"fault_density\"]].fillna(0.0)\n",
    "        corr = np.expm1(model.predict(X))  # predicted observed pga scale in training window\n",
    "        # Blend: geometric mean between prior and correction scale\n",
    "        out[\"pga_ml\"] = np.sqrt(out[\"pga_prior\"] * np.maximum(corr, 1e-6))\n",
    "        out[\"pga_g\"] = out[\"pga_ml\"].clip(lower=1e-6)\n",
    "    else:\n",
    "        out[\"pga_g\"] = out[\"pga_prior\"].clip(lower=1e-6)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# ----------------------------- EXPORT ---------------------------------\n",
    "\n",
    "def export_csv(df: pd.DataFrame, path: str):\n",
    "    df.to_csv(path, index=False)\n",
    "    print(f\"Saved {path}\")\n",
    "\n",
    "\n",
    "def export_geotiff(df: pd.DataFrame, cfg: Config, path: str):\n",
    "    if rasterio is None:\n",
    "        print(\"rasterio not available; skipping GeoTIFF export\")\n",
    "        return\n",
    "    # Build raster grid\n",
    "    res = cfg.GRID_RES_DEG\n",
    "    lats = np.arange(-90 + res/2, 90, res)\n",
    "    lons = np.arange(-180 + res/2, 180, res)\n",
    "    arr = np.full((len(lats), len(lons)), np.nan, dtype=np.float32)\n",
    "    # Map df values to raster\n",
    "    idx = {(round(r.lat,6), round(r.lon,6)): r.pga_g for r in df.itertuples(index=False)}\n",
    "    for i, lat in enumerate(lats[::-1]):  # top to bottom\n",
    "        for j, lon in enumerate(lons):\n",
    "            v = idx.get((round(lat,6), round(lon,6)), np.nan)\n",
    "            arr[i, j] = v\n",
    "    transform = from_origin(-180, 90, res, res)\n",
    "    with rasterio.open(\n",
    "        path,\n",
    "        \"w\",\n",
    "        driver=\"GTiff\",\n",
    "        height=arr.shape[0],\n",
    "        width=arr.shape[1],\n",
    "        count=1,\n",
    "        dtype=arr.dtype,\n",
    "        crs=\"EPSG:4326\",\n",
    "        transform=transform,\n",
    "        compress=\"lzw\",\n",
    "        nodata=np.nan,\n",
    "    ) as dst:\n",
    "        dst.write(arr, 1)\n",
    "    print(f\"Saved {path}\")\n",
    "\n",
    "\n",
    "# ------------------------------ MAIN ----------------------------------\n",
    "\n",
    "def main():\n",
    "    cfg = CFG\n",
    "    print(\"Loading data ...\")\n",
    "    eq, faults = load_data(cfg)\n",
    "    print(f\"Catalogue: {len(eq):,} events | Faults: {len(faults):,} features\")\n",
    "\n",
    "    print(\"Building global grid ...\")\n",
    "    grid = make_global_grid(cfg.GRID_RES_DEG)\n",
    "\n",
    "    print(\"Computing features (this can take time globally) ...\")\n",
    "    feat = build_features_for_grid(eq, faults, grid, cfg)\n",
    "\n",
    "    # Historical labels for ML correction (optional but recommended)\n",
    "    print(\"Creating historical labels for ML correction ...\")\n",
    "    try:\n",
    "        labels = make_training_labels(eq, grid, cfg, label_window_y=10)\n",
    "        if not labels.empty:\n",
    "            model = train_ml_correction(feat, labels)\n",
    "        else:\n",
    "            print(\"No labels were created; proceeding without ML correction.\")\n",
    "            model = None\n",
    "    except Exception as e:\n",
    "        print(\"Label creation failed:\", e)\n",
    "        model = None\n",
    "\n",
    "    # Forecast for each horizon\n",
    "    for H in cfg.HORIZONS:\n",
    "        year = cfg.REF_YEAR + H\n",
    "        print(f\"Forecasting PGA for horizon {H}y (≈ {year}) ...\")\n",
    "        pred = forecast_pga(feat, model, H, cfg)\n",
    "        pred.to_parquet(f\"/mnt/data/pga_features_{year}.parquet\", index=False)\n",
    "        export_csv(pred[[\"lat\",\"lon\",\"pga_g\"]], f\"/mnt/data/pga_forecast_{year}.csv\")\n",
    "        export_geotiff(pred[[\"lat\",\"lon\",\"pga_g\"]], cfg, f\"/mnt/data/pga_forecast_{year}.tif\")\n",
    "\n",
    "    print(\"Done.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b5bb4b-0aef-4e62-be93-55b122acbe39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenQuake not available — using built-in approximate GMPE fallback.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing PGA (PoE=10% in 50y):   0%|                                           | 26/259200 [00:20<45:43:52,  1.57it/s]"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# INSTALL NOTES (read only)\n",
    "# =========================\n",
    "# If you want the *real* GMPEs, install OpenQuake Engine (which contains hazardlib):\n",
    "#   conda install -c conda-forge openquake.engine\n",
    "# or:\n",
    "#   git clone --depth=1 https://github.com/gem/oq-engine.git\n",
    "#   cd oq-engine && python install.py devel   # then activate the created env\n",
    "#\n",
    "# This script will still run end-to-end without OpenQuake by using a built-in,\n",
    "# approximate GMPE fallback (clearly marked below).\n",
    "\n",
    "import os, math, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from tqdm import tqdm\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# ML\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "\n",
    "# Raster out\n",
    "import rasterio as rio\n",
    "from rasterio.transform import from_origin\n",
    "\n",
    "# ---------------- USER PATHS ----------------\n",
    "CSV = r\"D:/Earthquake_Project/Datasets/Earthquake_datasets.csv\"     # catalog csv\n",
    "FAULTS = r\"D:/Earthquake_Project/Datasets/Active faults/gem_active_faults.shp\"  # optional, not required to run\n",
    "OUT_DIR = r\"D:/Earthquake_Project\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# ---------------- SETTINGS ----------------\n",
    "MIN_MAG = 4.5         # filter small events\n",
    "MAX_DIST_KM = 300.0   # consider events within this distance for hazard\n",
    "GRID_RES_DEG = 0.5    # ~55 km at equator; use 0.25 for finer (slower)\n",
    "TARGET_POE = 0.10     # 10% in ...\n",
    "TARGET_YEARS = 50     # ... 50 years\n",
    "VS30 = 760.0          # \"rock\" site class\n",
    "RUP_DEPTH_DEFAULT = 8.0  # fallback rupture depth if missing\n",
    "\n",
    "# ---------------- 0) TRY OPENQUAKE; ELSE USE FALLBACK ----------------\n",
    "USE_OPENQUAKE = False\n",
    "try:\n",
    "    from openquake.hazardlib.gsim.boore_atkinson_2008 import BooreAtkinson2008\n",
    "    from openquake.hazardlib.const import StdDev\n",
    "    from openquake.hazardlib.imt import PGA\n",
    "    from openquake.hazardlib import contexts\n",
    "    gsim = BooreAtkinson2008()\n",
    "    imt = PGA()\n",
    "    USE_OPENQUAKE = True\n",
    "    print(\"Using OpenQuake Boore-Atkinson (2008) GMPE.\")\n",
    "except Exception as e:\n",
    "    print(\"OpenQuake not available — using built-in approximate GMPE fallback.\")\n",
    "    USE_OPENQUAKE = False\n",
    "\n",
    "# ---------------- 1) LOAD & CLEAN CATALOG ----------------\n",
    "if not os.path.exists(CSV):\n",
    "    raise FileNotFoundError(f\"Catalog not found: {CSV}\")\n",
    "\n",
    "eq = pd.read_csv(CSV)\n",
    "\n",
    "# normalize column names\n",
    "rename_map = {\n",
    "    \"latitude\":\"lat\", \"Latitude\":\"lat\",\n",
    "    \"longitude\":\"lon\", \"Longitude\":\"lon\",\n",
    "    \"depth\":\"depth_km\", \"Depth\":\"depth_km\", \"depth_km\":\"depth_km\",\n",
    "    \"mag\":\"mag\", \"magnitude\":\"mag\", \"Magnitude\":\"mag\",\n",
    "}\n",
    "for k,v in list(rename_map.items()):\n",
    "    if k in eq.columns:\n",
    "        eq = eq.rename(columns={k:v})\n",
    "\n",
    "# sanity checks\n",
    "needed = [\"lat\",\"lon\",\"mag\"]\n",
    "missing = [c for c in needed if c not in eq.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"CSV must contain columns {needed}; missing: {missing}\")\n",
    "\n",
    "eq = eq.dropna(subset=[\"lat\",\"lon\",\"mag\"]).copy()\n",
    "\n",
    "# parse time\n",
    "if \"time\" in eq.columns:\n",
    "    eq[\"time\"] = pd.to_datetime(eq[\"time\"], errors=\"coerce\", utc=True)\n",
    "else:\n",
    "    # fabricate a simple time if missing (assume all events in a 10-yr span)\n",
    "    eq[\"time\"] = pd.Timestamp(\"2000-01-01\", tz=\"UTC\") + pd.to_timedelta(np.arange(len(eq)) % 3650, unit=\"D\")\n",
    "eq = eq.dropna(subset=[\"time\"])\n",
    "\n",
    "# prefer Mw if provided\n",
    "if \"magType\" in eq.columns:\n",
    "    eq[\"magType\"] = eq[\"magType\"].astype(str)\n",
    "    mw_mask = eq[\"magType\"].str.upper().str.contains(\"MW\", na=False)\n",
    "    eq[\"mag_Mw\"] = np.where(mw_mask, eq[\"mag\"], np.nan)\n",
    "else:\n",
    "    eq[\"magType\"] = np.nan\n",
    "    eq[\"mag_Mw\"] = np.nan\n",
    "\n",
    "eq[\"mag_use\"] = eq[\"mag_Mw\"].fillna(eq[\"mag\"])\n",
    "\n",
    "# filter by magnitude\n",
    "eq = eq[eq[\"mag_use\"] >= MIN_MAG].copy()\n",
    "if eq.empty:\n",
    "    raise ValueError(\"No events remain after MIN_MAG filter.\")\n",
    "\n",
    "# duration of catalog (years)\n",
    "years_span = (eq[\"time\"].max() - eq[\"time\"].min()).days / 365.25\n",
    "years_span = max(1.0, float(years_span))\n",
    "\n",
    "# ensure depth\n",
    "if \"depth_km\" not in eq.columns:\n",
    "    eq[\"depth_km\"] = RUP_DEPTH_DEFAULT\n",
    "eq[\"depth_km\"] = eq[\"depth_km\"].fillna(RUP_DEPTH_DEFAULT)\n",
    "\n",
    "# GeoDataFrame\n",
    "eq_gdf = gpd.GeoDataFrame(\n",
    "    eq,\n",
    "    geometry=gpd.points_from_xy(eq[\"lon\"].astype(float), eq[\"lat\"].astype(float)),\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "# ---------------- 2) BUILD GLOBAL GRID ----------------\n",
    "lons = np.arange(-180 + GRID_RES_DEG/2, 180, GRID_RES_DEG)\n",
    "lats = np.arange(-90 + GRID_RES_DEG/2,   90, GRID_RES_DEG)\n",
    "grid_ll = [(lo, la) for la in lats for lo in lons]\n",
    "grid = pd.DataFrame(grid_ll, columns=[\"lon\",\"lat\"])\n",
    "grid_gdf = gpd.GeoDataFrame(grid, geometry=gpd.points_from_xy(grid[\"lon\"], grid[\"lat\"]), crs=\"EPSG:4326\")\n",
    "\n",
    "# ---------------- 3) HAVERSINE ----------------\n",
    "def haversine_km(lon1, lat1, lon2, lat2):\n",
    "    R = 6371.0\n",
    "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2.0)**2\n",
    "    return 2.0 * R * np.arcsin(np.sqrt(a))\n",
    "\n",
    "# ---------------- 4) GMPE WRAPPERS ----------------\n",
    "def _pga_ln_stats_openquake(M, Rrup_km, vs30=VS30, ztor=None, rake=None):\n",
    "    \"\"\"Mean ln(PGA) and TOTAL sigma via OpenQuake BA2008 (rock).\"\"\"\n",
    "    sctx = contexts.SitesContext()\n",
    "    sctx.vs30 = np.array([vs30], dtype=float)\n",
    "    sctx.vs30measured = np.array([False])\n",
    "    sctx.z1pt0 = np.array([np.nan])\n",
    "    sctx.z2pt5 = np.array([np.nan])\n",
    "\n",
    "    rctx = contexts.DistancesContext()\n",
    "    rctx.rrup = np.array([max(1.0, float(Rrup_km))], dtype=float)\n",
    "\n",
    "    rup = contexts.RuptureContext()\n",
    "    rup.mag = np.array([float(M)], dtype=float)\n",
    "    rup.ztor = np.array([ztor if ztor is not None else max(0.0, RUP_DEPTH_DEFAULT-2)], dtype=float)\n",
    "    rup.dip = np.array([90.0], dtype=float)\n",
    "    rup.width = np.array([np.nan])\n",
    "    rup.rake = np.array([rake if rake is not None else 0.0], dtype=float)\n",
    "\n",
    "    mean_ln, sigmas = gsim.get_mean_and_stddevs(sctx, rup, rctx, imt, [StdDev.TOTAL])\n",
    "    return float(mean_ln[0]), float(sigmas[0][0])\n",
    "\n",
    "def _pga_ln_stats_fallback(M, Rrup_km, vs30=VS30):\n",
    "    \"\"\"\n",
    "    APPROXIMATE fallback GMPE (rock, reference Vs30~760).\n",
    "    Form: ln(PGA[g]) = a + b*M - ln(R + h) - k*R\n",
    "    This is NOT a substitute for a vetted GMPE; it only keeps the pipeline running.\n",
    "    \"\"\"\n",
    "    a = -2.0      # baseline\n",
    "    b = 1.10      # magnitude scaling\n",
    "    h = 8.0       # pseudo-depth (km)\n",
    "    k = 0.0025    # geometric+anelastic decay\n",
    "    R = max(1.0, float(Rrup_km))\n",
    "    mean_ln = a + b*float(M) - math.log(R + h) - k*R\n",
    "    sigma = 0.6   # crude, total log std\n",
    "    return mean_ln, sigma\n",
    "\n",
    "def pga_ln_stats(M, Rrup_km, vs30=VS30, ztor=None, rake=None):\n",
    "    if USE_OPENQUAKE:\n",
    "        return _pga_ln_stats_openquake(M, Rrup_km, vs30, ztor, rake)\n",
    "    else:\n",
    "        return _pga_ln_stats_fallback(M, Rrup_km, vs30)\n",
    "\n",
    "def prob_IM_exceeds_x(M, R_km, x_g):\n",
    "    \"\"\"Lognormal exceedance probability P(IM > x | M,R).\"\"\"\n",
    "    if x_g <= 0:\n",
    "        return 1.0\n",
    "    mean_ln, sigma = pga_ln_stats(M, R_km)\n",
    "    z = (math.log(x_g) - mean_ln) / sigma\n",
    "    # 1 - Phi(z)\n",
    "    return 0.5 * (1.0 - math.erf(z / math.sqrt(2.0)))\n",
    "\n",
    "# ---------------- 5) HAZARD (ANNUAL RATE OF EXCEEDANCE) ----------------\n",
    "# Simple annualized rate per event: 1 / years_span, then sum contributions.\n",
    "EQ_LON = eq_gdf[\"lon\"].to_numpy()\n",
    "EQ_LAT = eq_gdf[\"lat\"].to_numpy()\n",
    "EQ_MAG = eq_gdf[\"mag_use\"].to_numpy()\n",
    "EQ_DEP = eq_gdf[\"depth_km\"].to_numpy()\n",
    "EVENT_RATE = 1.0 / years_span   # per-event annual rate\n",
    "\n",
    "def annual_exceedance_rate_at_threshold(lon, lat, x_g):\n",
    "    d = haversine_km(lon, lat, EQ_LON, EQ_LAT)\n",
    "    mask = d <= MAX_DIST_KM\n",
    "    if not np.any(mask):\n",
    "        return 0.0\n",
    "    # vectorized exceedance per masked events\n",
    "    d_m = d[mask]\n",
    "    m_m = EQ_MAG[mask]\n",
    "    # (Optional: include depth -> rrup ~ sqrt(r_hyp^2 + depth^2))\n",
    "    rrup = np.sqrt(d_m**2 + np.maximum(EQ_DEP[mask], 0.0)**2)\n",
    "    # compute probabilities\n",
    "    probs = np.array([prob_IM_exceeds_x(M, R, x_g) for M, R in zip(m_m, rrup)], dtype=float)\n",
    "    lam = float(np.sum(EVENT_RATE * probs))\n",
    "    return lam\n",
    "\n",
    "def invert_for_pga_poe(lon, lat, poe=TARGET_POE, years=TARGET_YEARS):\n",
    "    \"\"\"Find PGA[g] such that PoE in `years` equals `poe` via bisection on lambda.\"\"\"\n",
    "    target_lambda = -math.log(1.0 - float(poe)) / float(years)\n",
    "    # bracket (g)\n",
    "    lo, hi = 1e-4, 2.0\n",
    "    for _ in range(32):\n",
    "        mid = math.sqrt(lo*hi)\n",
    "        lam_mid = annual_exceedance_rate_at_threshold(lon, lat, mid)\n",
    "        if lam_mid > target_lambda:\n",
    "            hi = mid\n",
    "        else:\n",
    "            lo = mid\n",
    "    return hi  # conservative\n",
    "\n",
    "# ---------------- 6) COMPUTE PGA(10% in 50y) ON GRID ----------------\n",
    "vals = []\n",
    "tqdm_desc = f\"Computing PGA (PoE={TARGET_POE*100:.0f}% in {TARGET_YEARS}y)\"\n",
    "for i, row in tqdm(grid.iterrows(), total=len(grid), desc=tqdm_desc):\n",
    "    pga = invert_for_pga_poe(row[\"lon\"], row[\"lat\"], TARGET_POE, TARGET_YEARS)\n",
    "    vals.append(pga)\n",
    "grid[\"PGA_10in50_g\"] = vals\n",
    "\n",
    "# ---------------- 7) (OPTIONAL) FAULT FEATURES ----------------\n",
    "# Uncomment when you have a faults shapefile and want to add features\n",
    "# if os.path.exists(FAULTS):\n",
    "#     faults = gpd.read_file(FAULTS).to_crs(\"EPSG:4326\")\n",
    "#     # Reproject to an equal-area/equidistant CRS for distance if you need precision.\n",
    "#     # For speed, we do a crude lon/lat distance (~km) here:\n",
    "#     def dist_to_fault_km(pt):\n",
    "#         # quick min haversine to all fault vertices (coarse but fast)\n",
    "#         # better: project to local UTM and compute exact distance\n",
    "#         min_d = 1e9\n",
    "#         for geom in faults.geometry:\n",
    "#             if geom is None:\n",
    "#                 continue\n",
    "#             for x, y in getattr(geom, \"coords\", []):\n",
    "#                 d = haversine_km(pt.x, pt.y, x, y)\n",
    "#                 if d < min_d:\n",
    "#                     min_d = d\n",
    "#         return min_d\n",
    "#     grid_gdf[\"dist_fault_km\"] = grid_gdf.geometry.apply(dist_to_fault_km)\n",
    "# else:\n",
    "#     grid_gdf[\"dist_fault_km\"] = np.nan\n",
    "\n",
    "# ---------------- 8) TRAIN SIMPLE ML EMULATOR ----------------\n",
    "# Local features around each node:\n",
    "#   - rate proxy within 100km\n",
    "#   - mean depth within 200km\n",
    "#   - max M within 300km\n",
    "def local_features(lon, lat):\n",
    "    d = haversine_km(lon, lat, EQ_LON, EQ_LAT)\n",
    "    f1 = float(np.mean(d <= 100.0)) / years_span\n",
    "    f2 = float(np.mean(EQ_DEP[d <= 200.0])) if np.any(d <= 200.0) else RUP_DEPTH_DEFAULT\n",
    "    f3 = float(np.max(EQ_MAG[d <= 300.0])) if np.any(d <= 300.0) else MIN_MAG\n",
    "    return f1, f2, f3\n",
    "\n",
    "feat_rows = [local_features(lo, la) for lo, la in zip(grid[\"lon\"], grid[\"lat\"])]\n",
    "F = pd.DataFrame(feat_rows, columns=[\"rate100_ann\",\"meanDepth200_km\",\"maxMag300\"])\n",
    "F[\"lon\"] = grid[\"lon\"]; F[\"lat\"] = grid[\"lat\"]\n",
    "F[\"PGA_10in50_g\"] = grid[\"PGA_10in50_g\"]\n",
    "\n",
    "X = F[[\"rate100_ann\",\"meanDepth200_km\",\"maxMag300\",\"lon\",\"lat\"]].to_numpy()\n",
    "y = F[\"PGA_10in50_g\"].to_numpy()\n",
    "\n",
    "Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = xgb.XGBRegressor(\n",
    "    n_estimators=600, max_depth=8, learning_rate=0.05,\n",
    "    subsample=0.8, colsample_bytree=0.8, n_jobs=-1, random_state=42\n",
    ")\n",
    "model.fit(Xtr, ytr)\n",
    "pred = model.predict(Xte)\n",
    "print(f\"ML emulator R2: {r2_score(yte, pred):.3f}   MAE: {mean_absolute_error(yte, pred):.4f} g\")\n",
    "\n",
    "# Save trained model\n",
    "model.save_model(os.path.join(OUT_DIR, \"xgb_pga_emulator.json\"))\n",
    "\n",
    "# ---------------- 9) EXPORTS ----------------\n",
    "# CSV (features + PGA)\n",
    "csv_out = os.path.join(OUT_DIR, \"PGA_10in50_grid.csv\")\n",
    "F.to_csv(csv_out, index=False)\n",
    "\n",
    "# GeoTIFF (plate carrée, EPSG:4326)\n",
    "minx, maxx = -180.0, 180.0\n",
    "miny, maxy =  -90.0,  90.0\n",
    "res = float(GRID_RES_DEG)\n",
    "\n",
    "width  = int(round((maxx - minx) / res))\n",
    "height = int(round((maxy - miny) / res))\n",
    "arr = np.full((height, width), np.float32(np.nan))\n",
    "\n",
    "# index helpers\n",
    "lon_idx = np.clip(((F[\"lon\"].to_numpy() - minx) / res).round().astype(int), 0, width-1)\n",
    "lat_idx = np.clip(((maxy - F[\"lat\"].to_numpy()) / res).round().astype(int), 0, height-1)\n",
    "\n",
    "arr[lat_idx, lon_idx] = F[\"PGA_10in50_g\"].astype(\"float32\").to_numpy()\n",
    "\n",
    "transform = from_origin(minx, maxy, res, res)\n",
    "profile = {\n",
    "    \"driver\":\"GTiff\",\"height\":height,\"width\":width,\"count\":1,\"dtype\":\"float32\",\n",
    "    \"crs\":\"EPSG:4326\",\"transform\":transform,\"compress\":\"lzw\",\"nodata\":np.nan\n",
    "}\n",
    "tif_path = os.path.join(OUT_DIR, \"PGA_10in50_global.tif\")\n",
    "with rio.open(tif_path, \"w\", **profile) as dst:\n",
    "    dst.write(arr, 1)\n",
    "\n",
    "print(\"Wrote:\", tif_path, \"and\", csv_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833699cb-6446-4f4a-9897-eea850496019",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
